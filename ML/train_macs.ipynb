{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mac training Using Fluorescent and Brightfield Channels\n",
    "\n",
    "We train a predefined \"MacNet\" CNN to identify alveolar (tissue resident) macrophages versus bone marrow (proxy for monocyte-derived) macrophages. \n",
    "\n",
    "The input will be 1-4 channels of brightfield, lipid stain (BODIPY), nuclear stain (Hoechst), mitochondria stain (MitoTracker Red), or cell autofluorescence in green/red/blue channels.\n",
    "\n",
    "The output will be a binary classification of whether the cell is a bone marrow macrophage or alveolar macrophage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = [r\"..\\data\\processed\\first_batch\\alveolar.pickle\", \n",
    "         r\"..\\data\\processed\\first_batch\\marrow.pickle\",\n",
    "         r\"..\\data\\processed\\first_batch\\monocyte.pickle\"]\n",
    "num_classes = len(PATHS)\n",
    "input_ch = 1\n",
    "NUM_FOLDS = 5\n",
    "NUM_BATCHES = 4 # num samples in batch\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from MacDataset import MacDataset\n",
    "import macnet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "This section defines the transforms used to augment the base data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    standardize_input(),\n",
    "    rotate_90_input()\n",
    "    ])\n",
    "test_transforms = transforms.Compose([\n",
    "    standardize_input()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = []\n",
    "raw_labels = []\n",
    "for i, path in enumerate(PATHS):\n",
    "    path_data = pickle.load(open(path, \"rb\"))\n",
    "    path_data[\"labels\"][:] = i\n",
    "    raw_images.append(path_data[\"images\"])\n",
    "    raw_labels.append(path_data[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance \\# Images Per Class\n",
    "\n",
    "This balances the number of samples per class by randomly deleting samples till the classes are even. This seems to work better than the equal classes sampler\n",
    "\n",
    "It first detects the smallest class, then creates a random list of indices of which only the last min_len samples are kept. As this list is random, different samples should be selected each time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = sum([len(label) for label in raw_labels])\n",
    "balanced_images = []\n",
    "balanced_labels = []\n",
    "for i in range(len(raw_labels)):\n",
    "    num_samples = len(raw_labels[i])\n",
    "    if  num_samples < min_len:\n",
    "        min_len = num_samples\n",
    "\n",
    "for i in range(len(raw_labels)):\n",
    "    num_samples = len(raw_labels[i])\n",
    "    raw_idx = list(range(num_samples))\n",
    "    random.shuffle(raw_idx)\n",
    "    raw_idx = raw_idx[:min_len]\n",
    "    balanced_images.append(raw_images[i][raw_idx])\n",
    "    balanced_labels.append(raw_labels[i][raw_idx])\n",
    "\n",
    "images = np.vstack(balanced_images)\n",
    "labels = np.hstack(balanced_labels)\n",
    "\n",
    "fold_idx = list(range(len(labels)))\n",
    "random.shuffle(fold_idx)\n",
    "fold_idx = np.array_split(fold_idx, NUM_FOLDS)\n",
    "min_len = len(labels)\n",
    "for fold in fold_idx:\n",
    "    if len(fold) < min_len:\n",
    "        min_len = len(fold)\n",
    "\n",
    "for i in range(len(fold_idx)):\n",
    "    if len(fold_idx[i]) > min_len:\n",
    "        fold_idx[i] = np.delete(fold_idx[i], 0)\n",
    "\n",
    "i = 0\n",
    "testing_images = images[fold_idx[i]]\n",
    "training_images = np.delete(images, fold_idx[i], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, batches, output=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_done = 0\n",
    "    correct = 0\n",
    "    final_training_acc = 0\n",
    "    \n",
    "    for batch, data in enumerate(dataloader):\n",
    "        X, y = data[0][:,[0],:,:].to(device), data[1].to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(input=pred, target=y.to(torch.long))\n",
    "        # Backpropagation\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct += (y==torch.argmax(pred,1)).sum().item()\n",
    "        total_done += batches        \n",
    "        if batch % 25 == 0 and batch > 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "\n",
    "            training_acc = correct/total_done\n",
    "            final_training_acc = training_acc\n",
    "    print(f\"Avg. Loss: {loss:>7f}, Accuracy: {final_training_acc:>.2%} [{size:>5d}/{size:>5d}]\")   \n",
    "    print()\n",
    "    if output is not None:\n",
    "        output.append((loss.item(),final_training_acc))\n",
    "    return final_training_acc\n",
    "    \n",
    "\n",
    "def test(dataloader, model, acc_out, test_results=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            X, y = data[0][:,[0],:,:].to(device), data[1].to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(input=pred, target=y.to(torch.long)).item()\n",
    "            correct += (y==torch.argmax(pred,1)).sum().item()\n",
    "            if test_results is not None:\n",
    "                test_results.append((y.cpu(), pred.cpu()))\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"\\nTest Error: \\nAvg. Loss: {test_loss:>7f}, Accuracy: {correct:>0.2%}\\n\")\n",
    "\n",
    "    acc_out.append((test_loss, correct))\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_err_fold = []\n",
    "train_err_fold = []\n",
    "\n",
    "test_results_fold = [[] for _ in range(NUM_FOLDS)]\n",
    "train_hist = [[] for _ in range(NUM_FOLDS)]\n",
    "test_hist = [[] for _ in range(NUM_FOLDS)]\n",
    "\n",
    "for i in range(NUM_FOLDS):\n",
    "    print(\"\\n FOLD \" + str(i + 1) + \" OF \" + str(NUM_FOLDS))\n",
    "    print(\"=========================================================\\n\")\n",
    "    testing_images = images[fold_idx[i]]\n",
    "    testing_labels = labels[fold_idx[i]]\n",
    "    training_images = np.delete(images, fold_idx[i], axis=0)\n",
    "    training_labels = np.delete(labels, fold_idx[i], axis=0)\n",
    "    \n",
    "    train_data = MacDataset(training_images, training_labels, \n",
    "                                transform=train_transforms)\n",
    "    test_data = MacDataset(testing_images, testing_labels,\n",
    "                                transform=test_transforms)\n",
    "\n",
    "    train_sampler = equal_classes_sampler(train_data.labels)\n",
    "    test_sampler = equal_classes_sampler(test_data.labels)\n",
    "    \n",
    "    dataloader = DataLoader(train_data, batch_size=NUM_BATCHES, sampler=train_sampler,\n",
    "                            shuffle=False, num_workers=0)\n",
    "\n",
    "    dataloader_test = DataLoader(test_data, batch_size=NUM_BATCHES, sampler=test_sampler,\n",
    "                            shuffle=False, num_workers=0)              \n",
    "    # Get cpu or gpu device for training.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "    \n",
    "    model = macnet.Net(num_classes, input_ch).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    print(\"\\nTraining Start\")\n",
    "\n",
    "    training_error = []\n",
    "    testing_error = []\n",
    "    for t in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "        print(\"\\nTraining Error:\")\n",
    "        training_error.append(train(dataloader, model, loss_fn, optimizer, NUM_BATCHES, output=train_hist[i]))\n",
    "        if t < NUM_EPOCHS - 1:\n",
    "            testing_error.append(test(dataloader_test, model, acc_out=test_hist[i]))\n",
    "        else:\n",
    "            testing_error.append(test(dataloader_test, model, \n",
    "                                 acc_out=test_hist[i], \n",
    "                                 test_results=test_results_fold[i]))\n",
    "                                 \n",
    "    train_err_fold.append(training_error[-1])\n",
    "    curr_testing_error = testing_error[-1]\n",
    "\n",
    "    print(\"saving model\")\n",
    "    torch.save(model, \"./model_fold_\" + str(i))\n",
    "    pickle.dump(test_data, open(\"./test_data_fold_\" + str(i), \"wb\"))\n",
    "\n",
    "    test_err_fold.append(testing_error[-1])\n",
    "\n",
    "train_err_fold = [round(error, 4) for error in train_err_fold]\n",
    "test_err_fold = [round(error, 4) for error in test_err_fold]\n",
    "print(\"training errors per fold\")\n",
    "print(train_err_fold)\n",
    "print(\"testing errors per fold\")\n",
    "print(test_err_fold)\n",
    "print(\"saving training and testing data\")\n",
    "pickle.dump(test_results_fold, open(\"model_test_results.p\", \"wb\"))\n",
    "pickle.dump(train_hist, open(\"model_train_hist.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab8795380b30e625f05b9875eb19e47dede7d17a6e02ba200312899d03cb9f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('torchenv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
