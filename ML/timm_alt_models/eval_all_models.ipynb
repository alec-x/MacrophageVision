{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternate Macrophage Polarization Neural Net Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import seaborn as sn\n",
    "from scipy import ndimage\n",
    "import pickle\n",
    "from os import walk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "import copy\n",
    "from sklearn.manifold import TSNE\n",
    "from timm.models import create_model\n",
    "from timm.data import ImageDataset, create_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"..\\\\..\\\\data\\\\external\\\\mac_polar_test_res\\\\model_test_results.p\", \"rb\"))\n",
    "confusion_matrices = []\n",
    "matrices = []\n",
    "test_accuracies = []\n",
    "num_folds = len(data)\n",
    "arch_matrices = {}\n",
    "for fold in range(num_folds):\n",
    "    curr_data = list(zip(*data[fold]))\n",
    "    data[fold] = {}\n",
    "    data[fold][\"true\"] = curr_data[0]\n",
    "    data[fold][\"pred\"] = curr_data[1]\n",
    "\n",
    "    labels = data[fold][\"true\"]\n",
    "    labels = torch.flatten(torch.stack(labels))\n",
    "\n",
    "    outputs = data[fold][\"pred\"]\n",
    "    num_classes = len(outputs[0][0])\n",
    "    outputs = torch.flatten(torch.stack(outputs), end_dim=1)\n",
    "    outputs = torch.argmax(outputs, 1)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    confusion_matrices.append(np.zeros((num_classes, num_classes))) \n",
    "\n",
    "    for t, p in zip(labels, outputs):\n",
    "            confusion_matrices[fold][int(t), int(p)] += 1\n",
    "    matrix = np.array(confusion_matrices[fold])\n",
    "    matrix = np.array([i/sum(i) for i in matrix])\n",
    "    matrices.append(matrix)\n",
    "\n",
    "    test_accuracy = 0\n",
    "    for i in range(num_classes):\n",
    "        test_accuracy += confusion_matrices[fold][i,i]\n",
    "    test_accuracy = test_accuracy / sum(confusion_matrices[fold].flatten())\n",
    "    test_accuracies.append(test_accuracy)\n",
    "arch_matrices[\"macnet\"] = sum(confusion_matrices)\n",
    "\n",
    "print(f\"Average: Test Accuracy: {sum(test_accuracies)/num_folds:>.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archs = [\n",
    "    'efficientnet_b0', \n",
    "    'efficientnet_b2',\n",
    "    'inception_v4', \n",
    "    'pnasnet', \n",
    "    'resnext'\n",
    "]\n",
    "\n",
    "base_path = '..\\\\..\\\\data\\\\external\\\\mac_polar_test_res\\\\'\n",
    "\n",
    "data = {}\n",
    "for arch in archs:\n",
    "    arch_matrices \n",
    "    confusion_matrices = []\n",
    "    matrices = []\n",
    "    test_accuracies = []\n",
    "    num_folds = 5\n",
    "    data[arch] = []\n",
    "    for fold in range(num_folds):\n",
    "        path = \"\\\\\".join([base_path, arch, f\"fold_{fold+1}\"])\n",
    "        data[arch].append(pd.read_csv(path,header=None, names=[\"label\", \"pred\", \"M0\", \"M1\", \"M2\"]))\n",
    "        labels = np.array(data[arch][fold][\"label\"])\n",
    "\n",
    "        outputs = np.array(data[arch][fold][\"pred\"])\n",
    "        num_classes = int(np.max(labels) + 1)\n",
    "\n",
    "        confusion_matrices.append(np.zeros((num_classes, num_classes))) \n",
    "\n",
    "        for t, p in zip(labels, outputs):\n",
    "                confusion_matrices[fold][int(t), int(p)] += 1\n",
    "        \n",
    "        matrix = np.array(confusion_matrices[fold])\n",
    "        matrix = np.array([i/sum(i) for i in matrix])\n",
    "        matrices.append(matrix)\n",
    "\n",
    "        test_accuracy = 0\n",
    "        for i in range(num_classes):\n",
    "            test_accuracy += confusion_matrices[fold][i,i]\n",
    "        test_accuracy = test_accuracy / sum(confusion_matrices[fold].flatten())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "    data[arch] = pd.concat(data[arch])\n",
    "    arch_matrices[arch] = sum(confusion_matrices)\n",
    "\n",
    "    print(f\"{arch} Test Accuracy (avg): {sum(test_accuracies)/num_folds:>.2%}\")\n",
    "archs.append('macnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_labels = [\"M0\", \"M1\", \"M2\"]\n",
    "num_classes = len(axis_labels)\n",
    "\n",
    "for arch in archs:\n",
    "    curr_matrix = arch_matrices[arch]\n",
    "    curr_matrix_pct = np.array([i/sum(i) for i in curr_matrix])\n",
    "    matrix_df = pd.DataFrame(curr_matrix_pct, index=axis_labels, columns=axis_labels)\n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(matrix_df, annot=True, fmt='.2%') # font size\n",
    "    plt.title(f'M0/M1/M2 Classification Using {arch}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_data = []\n",
    "for arch in archs:\n",
    "    matrix_count = arch_matrices[arch]\n",
    "    FP = matrix_count.sum(axis=0) - np.diag(matrix_count)  \n",
    "    FN = matrix_count.sum(axis=1) - np.diag(matrix_count)\n",
    "    TP = np.diag(matrix_count)\n",
    "    TN = matrix_count.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    # F1 score\n",
    "    F1 =  2 * (PPV * TPR) / (PPV + TPR)\n",
    "    #print(PPV, TPR, F1, ACC)\n",
    "\n",
    "\n",
    "    columns = {\"Sensitivity\": np.mean(TPR), \n",
    "            \"Precision\": np.mean(TNR), \n",
    "            \"F1\": np.mean(F1), \n",
    "            \"Accuracy\": np.mean(ACC)}\n",
    "    \n",
    "    for key in columns.keys():\n",
    "        type_data = key\n",
    "        value = columns[key]\n",
    "        roc_data.append([arch, type_data, value])\n",
    "        \n",
    "roc_data = pd.DataFrame(roc_data, columns=[\"class\", \"data_type\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.set_theme(style=\"whitegrid\")\n",
    "\n",
    "g = sn.catplot(\n",
    "    data=pd.DataFrame(roc_data), kind=\"bar\",\n",
    "    x=\"class\", y=\"value\", hue=\"data_type\",\n",
    "    ci=\"sd\", palette=\"dark\", height=6\n",
    ")\n",
    "g.set(ylim=(0, 1))\n",
    "g.set_axis_labels(\"label\", \"Percentage\")\n",
    "g.legend.set_title(\"Metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefining standardization operation used in training because cannot get transforms.Compose to work properly with Dataset object. TODO: Fixable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_input(image):\n",
    "    image = image.detach().numpy()\n",
    "    chans = range(image.shape[1])\n",
    "    means = [np.mean(image[0][chan]) for chan in chans]\n",
    "    stdevs = [np.std(image[0][chan]) for chan in chans]\n",
    "    for chan in chans:\n",
    "        image[0][chan] = (image[0][chan] - means[chan]) / stdevs[chan]\n",
    "    \n",
    "    output = torch.Tensor(image)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate fluorescent intensities of stains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intensity(img, channel_num, diag=False):\n",
    "    arr = np.copy(img[channel_num])\n",
    "    avg1 = list(arr[:2,:].flatten())\n",
    "    avg2 = list(arr[:,:2].flatten())\n",
    "    avg3 = list(arr[-2:,:].flatten())\n",
    "    avg4 = list(arr[:,-2:].flatten())\n",
    "    avgs = avg1 + avg2 + avg3 + avg4\n",
    "    avgs.sort()\n",
    "    avg = avgs[int(len(avgs)*0.9)] # 70th percentile\n",
    "\n",
    "    \n",
    "    arr2 = np.copy(arr)\n",
    "    arr2 = arr2 - avg\n",
    "    arr2[arr2 < 0] = 0\n",
    "    arr2 = ndimage.median_filter(arr2, size=3)\n",
    "    num_non_zero = np.count_nonzero(arr2)\n",
    "    num_total = np.sum(arr2)\n",
    "    avg2 = num_total / num_non_zero\n",
    "    \n",
    "    lit_pct = num_non_zero/(96*96)*100\n",
    "\n",
    "    if diag:\n",
    "        print(\"average intensity of lit pixels: \", round(avg2,2))\n",
    "        print(\"percentage \\\"lit\\\": \", round(lit_pct, 2))\n",
    "        toshow = [arr, arr2]\n",
    "        labels = [\"Stain\", \"Clean Stain\"]\n",
    "        num_show = len(toshow)\n",
    "        f, axarr = plt.subplots(1,num_show, figsize=(8, 4))\n",
    "        for i in range(num_show):\n",
    "            axarr[i].imshow(toshow[i])\n",
    "            axarr[i].grid(False)\n",
    "            axarr[i].set_title(labels[i]) \n",
    "            axarr[i].get_xaxis().set_visible(False)\n",
    "            axarr[i].get_yaxis().set_visible(False)\n",
    "\n",
    "        plt.show()\n",
    "    return avg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get CD80 and CD206 images and calculate fluorescent intensity\n",
    "\n",
    "Not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    fluor_marks_temp = {}\n",
    "    fluor_paths = {\n",
    "        ('M0', r'../../data/processed/kerryn_dec/M0.pickle'),\n",
    "        ('M1', r'../../data/processed/kerryn_dec/M1.pickle'),\n",
    "        ('M2', r'../../data/processed/kerryn_dec/M2.pickle')\n",
    "    }\n",
    "    for ele in fluor_paths:\n",
    "        data = pickle.load(open(ele[1], 'rb'))\n",
    "        fluor_marks_temp[ele[0]] = data['images'][:,2:4,:,:]\n",
    "\n",
    "    # Get sample order in each fold. This could be made cleaner by re-organizing everything from the start\n",
    "    fluor_marks = {}\n",
    "    for test_fold in range(1,6):\n",
    "        test_path = '../../data/processed/dataset_split/fold_%i/test' % test_fold\n",
    "        fluor_marks[test_fold] = {}\n",
    "        for pheno in ['M0', 'M1', 'M2']: # Clean this up later\n",
    "            fluor_marks[test_fold][pheno] = {}\n",
    "            curr_path = f'{test_path}/{pheno}'\n",
    "            (_, _, curr_files) = next(walk(curr_path))\n",
    "            curr_files = [int(file.rstrip('.png')) for file in curr_files]\n",
    "            for file_num in curr_files:\n",
    "                fluor_marks[test_fold][pheno][file_num] = fluor_marks_temp[pheno][file_num]\n",
    "\n",
    "    fluorescent_stain = {}\n",
    "    thresh = 8\n",
    "    # 0 for CD80(blue), 1 for CD206(red), 2 for both (purple), 3 for neither (grey)\n",
    "\n",
    "    # Need to get these results per fold then apply it to the fold tested\n",
    "    for test_fold in range(1,6):\n",
    "        fluorescent_stain[test_fold] = {}\n",
    "\n",
    "        for pheno in ['M0', 'M1', 'M2']: # Clean this up later\n",
    "            fluorescent_stain[test_fold][pheno] = []\n",
    "\n",
    "            for sample_num in fluor_marks[test_fold][pheno]:\n",
    "                CD80_brightness = calculate_intensity(fluor_marks[test_fold][pheno][sample_num], 0)\n",
    "                CD206_brightness = calculate_intensity(fluor_marks[test_fold][pheno][sample_num], 1)\n",
    "                \n",
    "\n",
    "                cond_1 = CD80_brightness > thresh\n",
    "                cond_2 = CD206_brightness > thresh\n",
    "                if cond_1 and cond_2:\n",
    "                    fluorescent_stain[test_fold][pheno].append(2)\n",
    "                elif cond_1:\n",
    "                    fluorescent_stain[test_fold][pheno].append(0)\n",
    "                elif cond_2:\n",
    "                    fluorescent_stain[test_fold][pheno].append(1)\n",
    "                else:\n",
    "                    fluorescent_stain[test_fold][pheno].append(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Extract Feature Map\n",
    "Define calculation for extracting CD80 and CD206 levels from stains. Corrective techniques applied to remove image noise and variance between light levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = {}\n",
    "model_path['efficientnet_b0'] = r'.\\output\\train\\20220217-212448-efficientnet_b0-96\\model_best.pth.tar' \n",
    "model_path['efficientnet_b2'] = r'.\\output\\train\\20220217-222826-efficientnet_b2-96\\model_best.pth.tar'\n",
    "model_path['inception_v4'] =    r'.\\output\\train\\20220218-000119-inception_v4-96\\model_best.pth.tar'\n",
    "model_path['pnasnet5large'] =   r'.\\output\\train\\20220218-021523-pnasnet5large-96\\model_best.pth.tar'\n",
    "model_path['resnext101_32x8d'] =r'.\\output\\train\\20220218-103054-resnext101_32x8d-96\\model_best.pth.tar'\n",
    "\n",
    "tsne_results = {}\n",
    "base_path = '..\\\\..\\\\data\\\\external\\\\mac_polar_test_res\\\\'\n",
    "\n",
    "for arch in archs:    \n",
    "    print('Processing ' + arch)\n",
    "    test_path = \"\\\\\".join([base_path, arch, f\"fold_1\"])\n",
    "    num_classes = 3\n",
    "    model = create_model(\n",
    "        arch,\n",
    "        num_classes=num_classes,\n",
    "        in_chans=3,\n",
    "        pretrained=False,\n",
    "        checkpoint_path=model_path[arch])\n",
    "    model = model.cuda()\n",
    "\n",
    "    loader = create_loader(\n",
    "        ImageDataset(test_path),\n",
    "        input_size=(3,96,96),\n",
    "        batch_size=1,\n",
    "        use_prefetcher=True,\n",
    "        interpolation='bicubic',\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "        num_workers=1,\n",
    "        no_aug=True\n",
    "        )\n",
    "    model.eval()\n",
    "        \n",
    "    feature_maps = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(loader):\n",
    "            input = input.cuda()\n",
    "            feature_maps.append(model.forward_features(input).cpu().numpy())\n",
    "            targets.append(target.cpu())\n",
    "\n",
    "    feature_maps = np.stack(feature_maps)\n",
    "    num_samples = feature_maps.shape[0] * feature_maps.shape[1]\n",
    "    features = feature_maps.shape[2:]\n",
    "    feature_maps = np.reshape(feature_maps, (num_samples,) + features)\n",
    "    feature_maps = np.reshape(feature_maps, (num_samples, reduce(mul, features)))\n",
    "    \n",
    "    targets = np.stack(targets)\n",
    "    targets = np.reshape(targets, (num_samples))\n",
    "\n",
    "    scaled_data = StandardScaler().fit_transform(feature_maps)\n",
    "    TSNE = TSNE(n_components=2, perplexity=40, n_iter=10000, learning_rate=200)\n",
    "    tsne_results = TSNE.fit_transform(scaled_data)\n",
    "    df_tsne = pd.DataFrame(tsne_results, columns=['t-sne-one', 't-sne-two'])\n",
    "    phenotypes = {0:\"M0\", 1:\"M1\", 2:\"M2\"}\n",
    "    df_tsne['label'] = [phenotypes[int(ele)] for ele in targets]\n",
    "    tsne_results[arch] = df_tsne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "num_archs = len(archs)\n",
    "# fig, ax = plt.subplots(1, num_archs, figsize=(10*num_archs+5,10))\n",
    "\n",
    "\n",
    "for i, arch in enumerate(archs):\n",
    "    fig, ax =plt.subplots(1,2, figsize=(20,10))\n",
    "\n",
    "    colors = [\"#0a70c4\", \"#db0d0d\", \"#660ddb\", \"#b5b5b5\"]\n",
    "    customPalette = sns.set_palette(sns.color_palette(colors))\n",
    "    sns.scatterplot(\n",
    "        x=\"umap-one\", y=\"umap-two\",\n",
    "        hue=\"fluor_mark\",\n",
    "        palette=customPalette,\n",
    "        data=umap_results[arch],\n",
    "        hue_order = ['CD80+', 'CD206+', 'CD80+/CD206+', 'CD80-/CD206-'],\n",
    "        legend=\"full\",\n",
    "        alpha=0.5,\n",
    "        ax=ax[0],\n",
    "    )\n",
    "    ax[0].set_title(\"Fluorescent Marker Phenotype\")\n",
    "    colors = [\"#b5b5b5\", \"#0a70c4\", \"#db0d0d\"]\n",
    "    customPalette = sns.set_palette(sns.color_palette(colors))         \n",
    "    sns.scatterplot(\n",
    "        x=\"umap-one\", y=\"umap-two\",\n",
    "        hue=\"label\",\n",
    "        data=umap_results[arch],\n",
    "        palette=customPalette,\n",
    "        hue_order = ['M0', 'M1', 'M2'],\n",
    "        legend=\"full\",\n",
    "        alpha=0.5,\n",
    "        ax=ax[1]\n",
    "    )\n",
    "    ax[1].set_title(\"UMAP for \" + arch)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab8795380b30e625f05b9875eb19e47dede7d17a6e02ba200312899d03cb9f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('torchenv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
