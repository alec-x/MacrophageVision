{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_path = './model_fold_0'\n",
    "net = torch.load(model_path)\n",
    "net.to(\"cpu\")\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = []\n",
    "PATHS.append(r\"..\\data\\processed\\kerryn_dec\\M0.pickle\")\n",
    "PATHS.append(r\"..\\data\\processed\\kerryn_dec\\M1.pickle\")\n",
    "PATHS.append(r\"..\\data\\processed\\kerryn_dec\\M2.pickle\")\n",
    "         \n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from utils import *\n",
    "transforms = transforms.Compose([\n",
    "    standardize_input()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from MacDataset import MacDataset\n",
    "raw_images = []\n",
    "raw_labels = []\n",
    "for i, path in enumerate(PATHS):\n",
    "    path_data = pickle.load(open(path, \"rb\"))\n",
    "    path_data[\"labels\"][:] = i\n",
    "    raw_images.append(path_data[\"images\"])\n",
    "    raw_labels.append(path_data[\"labels\"])\n",
    "\n",
    "images = np.vstack(raw_images)[:,:,:,:]\n",
    "labels = np.hstack(raw_labels)\n",
    "\n",
    "testset = MacDataset(images, labels, \n",
    "                            transform=transforms)\n",
    "test_sampler = equal_classes_sampler(testset.labels)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, sampler=test_sampler,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate CD80/CD206 Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def imshow(img, transpose = True):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize = (11,4))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[:,[0],:,:]))\n",
    "\n",
    "l_names = [\"M0\", \"M1\", \"M2\"]\n",
    "print('Ground Truth   : ', ' '.join('%15s' % l_names[int(labels[j].item())] \n",
    "                            for j in range(BATCH_SIZE)))\n",
    "\n",
    "pred_names = [\"M0\", \"M1\", \"M2\"]\n",
    "outputs = net(images[:,0:2,:,:].float())\n",
    "predicted = torch.argmax(outputs,1).to(torch.double)\n",
    "probability = torch.softmax(outputs, 1).detach().numpy()\n",
    "\n",
    "print('Predicted      : ', ' '.join('%15s' % pred_names[int(predicted[j].item())] \n",
    "                            for j in range(BATCH_SIZE)))\n",
    "print('M0 prob        : ', ' '.join('%15.5f' % probability[j][0].item()\n",
    "                            for j in range(BATCH_SIZE)))\n",
    "\n",
    "print('M1 prob        : ', ' '.join('%15.5f' % probability[j][1].item()\n",
    "                            for j in range(BATCH_SIZE)))            \n",
    "print('M2 prob        : ', ' '.join('%15.5f' % probability[j][2].item()\n",
    "                            for j in range(BATCH_SIZE)))                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "def confusion_matrix(out, test_labels):\n",
    "    # out = alveolar, marrow, monocyte\n",
    "    # labels = alv\n",
    "    num_classes_out = max(out) + 1\n",
    "    num_classes_test = max(test_labels) + 1\n",
    "    \n",
    "    confusion_matrix = np.zeros((num_classes_out, num_classes_test))\n",
    "\n",
    "    for t, p in zip(out, test_labels):\n",
    "        confusion_matrix[int(t), int(p)] += 1\n",
    "\n",
    "    matrix = np.array(confusion_matrix)\n",
    "    matrix_2 = np.array([i/sum(i) for i in matrix])\n",
    "\n",
    "    return matrix_2, matrix\n",
    "\n",
    "def show_matrix(matrix, matrix_2, labels_index, labels_col):\n",
    "\n",
    "    matrix_df = pd.DataFrame(matrix, index=labels_index, columns=labels_col)\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                matrix_2.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     matrix.flatten()]\n",
    "    class_labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "          zip(group_percentages, group_counts)]\n",
    "\n",
    "    class_labels = np.asarray(class_labels).reshape(matrix.shape)\n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(matrix_df, annot=class_labels, fmt=\"\") # font size\n",
    "    plt.title(\"Confusion Matrix of CNN on New Dataset\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "preds = np.array([])\n",
    "truths = np.array([])\n",
    "for data in test_loader:\n",
    "    X, y = data[0][:,0:2,:,:].to(\"cpu\"), data[1].to(\"cpu\")\n",
    "    pred = net(X.float())\n",
    "    pred = torch.argmax(pred,1).detach().numpy()\n",
    "    y = y.detach().numpy()\n",
    "    preds = np.append(preds, pred)\n",
    "    truths = np.append(truths, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_index = [\"M0\", \"M1\", \"M2\"]\n",
    "#labels_col = [\"alveolar\", \"marrow\", \"alveolar_monocyte\", \"marrow_monocyte\"]\n",
    "labels_col = [\"M0\", \"M1\", \"M2\"]\n",
    "matrix, numbers = confusion_matrix(preds.astype(int), truths.astype(int))\n",
    "#show_matrix(matrix, labels_index, labels_col)\n",
    "show_matrix(matrix, numbers, labels_index, labels_col)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "491765e01fa3dfc6c98fe3165a4d0b07724f880971dca49810d2604a1e334570"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('MacVis2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
